# TITAN AI

Welcome to **TITAN AI** â€“ a powerful AI-driven system aimed at transforming [brief purpose or domain of project if known, e.g., "legal document analysis" or "health diagnostics"].


## ğŸ“· Screenshot

![Model UI Demo](https://github.com/manav8826/TITAN_AI/blob/main/Screenshot%202025-06-22%20161603.png?raw=true)
![Model UI Demo](https://github.com/manav8826/TITAN_AI/blob/main/Screenshot%202025-06-22%20161615.png?raw=true)




## ğŸš€ Features

- âœ… [Feature 1 â€“ e.g., Intelligent Document Processing]
- âœ… [Feature 2 â€“ e.g., Natural Language Understanding]
- âœ… [Feature 3 â€“ e.g., Real-Time Inference or Analytics]
- âœ… Modular codebase for easy customization

## ğŸ“‚ Project Structure
TitanAI-2025-master/
â”œâ”€â”€ data/ # Datasets or input files
â”œâ”€â”€ models/ # Pre-trained or custom ML/DL models
â”œâ”€â”€ scripts/ # Python scripts for training/inference
â”œâ”€â”€ utils/ # Helper functions and utilities
â”œâ”€â”€ app.py # Main application logic
â”œâ”€â”€ requirements.txt # Python dependencies
â””â”€â”€ README.md # This file

## âš™ï¸ Setup Instructions

1. **Clone the Repository**
   ```bash
   git clone https://github.com/manav8826/TITAN_AI.git
   cd TITAN_AI/TitanAI-2025-master
2. Create Virtual Environment (Optional but Recommended)

bash
python -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate


3.Install Dependencies
pip install -r requirements.txt



4.Run the App
python app.py


ğŸ§  Models Used
Model 1: Convolutional Neural Network (CNN) â€” used for feature extraction and classification

Model 2: LSTM (Long Short-Term Memory) â€” used for sequence modeling and time-series data

Model 3: Wav2Vec2 â€” a transformer-based model for audio and speech analysis

Model 4: HuBERT â€” self-supervised learning model for audio representations

These models were trained and tested on a combination of public datasets such as:

RAVDESS (Ryerson Audio-Visual Database of Emotional Speech and Song)

CREMA-D (Crowd-sourced Emotional Multimodal Actors Dataset)

TESS (Toronto Emotional Speech Set)

SAVEE (Surrey Audio-Visual Expressed Emotion)


ğŸ“Š Results / Demo
Accuracy:

CNN: 81.2%

CNN-LSTM: 83.6%

Wav2Vec2: 89.7%

HuBERT: 91.3%

Evaluation Metrics:

Precision, Recall, F1-Score (reported per emotion class)


ğŸ¤ Contributing
Pull requests are welcome!
If you wish to contribute:

Fork this repo

Create a new branch (git checkout -b feature-name)

Commit your changes

Push to the branch

Create a Pull Request

Please open an issue first to discuss any major changes.

ğŸ“œ License
This project is licensed under the MIT License.
You are free to use, modify, and distribute it under the terms of the license.

ğŸ™‹â€â™‚ï¸ Author
Manav Gupta
ğŸ“ Noida, India
ğŸ”— https://www.linkedin.com/in/manav-gupta-803a1825a/
ğŸ“§ manavgupta8527@gmail.com



