# TITAN AI

Welcome to **TITAN AI** – a powerful AI-driven system aimed at transforming [brief purpose or domain of project if known, e.g., "legal document analysis" or "health diagnostics"].


## 📷 Screenshot

![Model UI Demo](https://github.com/manav8826/TITAN_AI/blob/main/Screenshot%202025-06-22%20161603.png?raw=true)
![Model UI Demo](https://github.com/manav8826/TITAN_AI/blob/main/Screenshot%202025-06-22%20161615.png?raw=true)




## 🚀 Features

- ✅ [Feature 1 – e.g., Intelligent Document Processing]
- ✅ [Feature 2 – e.g., Natural Language Understanding]
- ✅ [Feature 3 – e.g., Real-Time Inference or Analytics]
- ✅ Modular codebase for easy customization

## 📂 Project Structure
TitanAI-2025-master/
├── data/ # Datasets or input files
├── models/ # Pre-trained or custom ML/DL models
├── scripts/ # Python scripts for training/inference
├── utils/ # Helper functions and utilities
├── app.py # Main application logic
├── requirements.txt # Python dependencies
└── README.md # This file

## ⚙️ Setup Instructions

1. **Clone the Repository**
   ```bash
   git clone https://github.com/manav8826/TITAN_AI.git
   cd TITAN_AI/TitanAI-2025-master
2. Create Virtual Environment (Optional but Recommended)

bash
python -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate


3.Install Dependencies
pip install -r requirements.txt



4.Run the App
python app.py


🧠 Models Used
Model 1: Convolutional Neural Network (CNN) — used for feature extraction and classification

Model 2: LSTM (Long Short-Term Memory) — used for sequence modeling and time-series data

Model 3: Wav2Vec2 — a transformer-based model for audio and speech analysis

Model 4: HuBERT — self-supervised learning model for audio representations

These models were trained and tested on a combination of public datasets such as:

RAVDESS (Ryerson Audio-Visual Database of Emotional Speech and Song)

CREMA-D (Crowd-sourced Emotional Multimodal Actors Dataset)

TESS (Toronto Emotional Speech Set)

SAVEE (Surrey Audio-Visual Expressed Emotion)


📊 Results / Demo
Accuracy:

CNN: 81.2%

CNN-LSTM: 83.6%

Wav2Vec2: 89.7%

HuBERT: 91.3%

Evaluation Metrics:

Precision, Recall, F1-Score (reported per emotion class)


🤝 Contributing
Pull requests are welcome!
If you wish to contribute:

Fork this repo

Create a new branch (git checkout -b feature-name)

Commit your changes

Push to the branch

Create a Pull Request

Please open an issue first to discuss any major changes.

📜 License
This project is licensed under the MIT License.
You are free to use, modify, and distribute it under the terms of the license.

🙋‍♂️ Author
Manav Gupta
📍 Noida, India
🔗 https://www.linkedin.com/in/manav-gupta-803a1825a/
📧 manavgupta8527@gmail.com



